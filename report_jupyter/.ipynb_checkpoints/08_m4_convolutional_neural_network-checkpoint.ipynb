{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project: Arxiv Classifier\n",
    "Authors: Amir Yunus, Brandon Gay, Lee Oon Teng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, split_data, save_model\n",
    "from utils.architecture.convolutional_neural_network import ConvolutionalNeuralNetwork\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - $M_4$: Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08.1 - Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_emb_0</th>\n",
       "      <th>title_emb_1</th>\n",
       "      <th>title_emb_2</th>\n",
       "      <th>title_emb_3</th>\n",
       "      <th>title_emb_4</th>\n",
       "      <th>title_emb_5</th>\n",
       "      <th>title_emb_6</th>\n",
       "      <th>title_emb_7</th>\n",
       "      <th>title_emb_8</th>\n",
       "      <th>title_emb_9</th>\n",
       "      <th>...</th>\n",
       "      <th>title_sentiment</th>\n",
       "      <th>summary_sentiment</th>\n",
       "      <th>comment_sentiment</th>\n",
       "      <th>authors_sentiment</th>\n",
       "      <th>title_ari</th>\n",
       "      <th>summary_ari</th>\n",
       "      <th>comment_ari</th>\n",
       "      <th>authors_ari</th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.316280</td>\n",
       "      <td>0.338162</td>\n",
       "      <td>0.180578</td>\n",
       "      <td>-0.086113</td>\n",
       "      <td>-0.247716</td>\n",
       "      <td>0.640725</td>\n",
       "      <td>0.333832</td>\n",
       "      <td>0.832219</td>\n",
       "      <td>-0.262952</td>\n",
       "      <td>0.072209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.451058</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>physics</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.271645</td>\n",
       "      <td>0.228114</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>-0.257775</td>\n",
       "      <td>-0.083051</td>\n",
       "      <td>0.499957</td>\n",
       "      <td>0.471172</td>\n",
       "      <td>0.737172</td>\n",
       "      <td>-0.135142</td>\n",
       "      <td>0.213080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530556</td>\n",
       "      <td>0.406599</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>physics</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.380401</td>\n",
       "      <td>0.264061</td>\n",
       "      <td>0.162928</td>\n",
       "      <td>-0.187630</td>\n",
       "      <td>-0.171296</td>\n",
       "      <td>0.660988</td>\n",
       "      <td>0.372280</td>\n",
       "      <td>0.938673</td>\n",
       "      <td>-0.259530</td>\n",
       "      <td>-0.110329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.529045</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>physics</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.496275</td>\n",
       "      <td>0.129620</td>\n",
       "      <td>0.048845</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>-0.320589</td>\n",
       "      <td>0.458095</td>\n",
       "      <td>0.439276</td>\n",
       "      <td>0.694186</td>\n",
       "      <td>-0.358091</td>\n",
       "      <td>-0.272360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.441523</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mathematics</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.163396</td>\n",
       "      <td>0.336321</td>\n",
       "      <td>0.343406</td>\n",
       "      <td>-0.162484</td>\n",
       "      <td>-0.134234</td>\n",
       "      <td>0.729687</td>\n",
       "      <td>0.424456</td>\n",
       "      <td>0.891292</td>\n",
       "      <td>-0.304655</td>\n",
       "      <td>0.020353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.472394</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>physics</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   title_emb_0  title_emb_1  title_emb_2  title_emb_3  title_emb_4  \\\n",
       "0    -0.316280     0.338162     0.180578    -0.086113    -0.247716   \n",
       "1    -0.271645     0.228114     0.203795    -0.257775    -0.083051   \n",
       "2    -0.380401     0.264061     0.162928    -0.187630    -0.171296   \n",
       "3    -0.496275     0.129620     0.048845     0.000629    -0.320589   \n",
       "4    -0.163396     0.336321     0.343406    -0.162484    -0.134234   \n",
       "\n",
       "   title_emb_5  title_emb_6  title_emb_7  title_emb_8  title_emb_9  ...  \\\n",
       "0     0.640725     0.333832     0.832219    -0.262952     0.072209  ...   \n",
       "1     0.499957     0.471172     0.737172    -0.135142     0.213080  ...   \n",
       "2     0.660988     0.372280     0.938673    -0.259530    -0.110329  ...   \n",
       "3     0.458095     0.439276     0.694186    -0.358091    -0.272360  ...   \n",
       "4     0.729687     0.424456     0.891292    -0.304655     0.020353  ...   \n",
       "\n",
       "   title_sentiment  summary_sentiment  comment_sentiment  authors_sentiment  \\\n",
       "0         0.600000           0.451058               0.50                0.5   \n",
       "1         0.530556           0.406599               0.50                0.5   \n",
       "2         0.608333           0.529045               0.70                0.5   \n",
       "3         0.312500           0.441523               0.65                0.5   \n",
       "4         0.490000           0.472394               0.50                0.5   \n",
       "\n",
       "   title_ari  summary_ari  comment_ari  authors_ari     category  split  \n",
       "0        1.0          1.0          1.0          1.0      physics  train  \n",
       "1        1.0          1.0          1.0          1.0      physics  train  \n",
       "2        1.0          1.0          1.0          1.0      physics  train  \n",
       "3        1.0          1.0          1.0          1.0  mathematics  train  \n",
       "4        1.0          1.0          1.0          1.0      physics  train  \n",
       "\n",
       "[5 rows x 3158 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(58816, 3158)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'arxiv'\n",
    "df = load_data(f'../data/03_{filename}_engineered.bin')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08.2 - Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 37142 samples\n",
      "Validation set: 15795 samples\n",
      "Test set: 5879 samples\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08.3 - Create the Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 GPUs!\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "m4_model = ConvolutionalNeuralNetwork(input_dim=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08.4 - Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py\", line 213, in forward\n    return F.max_pool2d(\n           ^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/_jit_internal.py\", line 624, in fn\n    return if_false(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 830, in _max_pool2d\n    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 1 has a total capacity of 39.38 GiB of which 7.81 MiB is free. Process 1420339 has 38.50 GiB memory in use. Process 1760366 has 880.00 MiB memory in use. Of the allocated memory 240.53 MiB is allocated by PyTorch, and 15.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mm4_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2_500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/report_jupyter/utils/architecture/convolutional_neural_network.py:197\u001b[0m, in \u001b[0;36mConvolutionalNeuralNetwork.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val, batch_size, epochs)\u001b[0m\n\u001b[1;32m    194\u001b[0m batch_X, batch_y \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch_y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, batch_y)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Calculate training accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/report_jupyter/utils/architecture/convolutional_neural_network.py:116\u001b[0m, in \u001b[0;36mConvolutionalNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mside_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mside_length)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Forward pass through conv layers\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m    119\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:126\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 126\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py\", line 213, in forward\n    return F.max_pool2d(\n           ^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/_jit_internal.py\", line 624, in fn\n    return if_false(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jovyan/text_web/IS6751_project_arxiv_classifier/venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 830, in _max_pool2d\n    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 1 has a total capacity of 39.38 GiB of which 7.81 MiB is free. Process 1420339 has 38.50 GiB memory in use. Process 1760366 has 880.00 MiB memory in use. Of the allocated memory 240.53 MiB is allocated by PyTorch, and 15.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = m4_model.fit(X_train, y_train, X_val, y_val, batch_size = 20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08.5 - Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_model.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08.6 - Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(m4_model, '../models/m4_convolutional_neural_network.bin')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
